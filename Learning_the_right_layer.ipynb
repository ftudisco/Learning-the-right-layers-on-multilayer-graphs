{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866b3dc8-f383-4a8d-9b6a-2d66ac54b8c2",
   "metadata": {},
   "source": [
    "## Learning the right layers: a data-driven semi-supervised learning model for multilayer graphs - SYNTHETIC DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3035b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn\n",
    "import scipy\n",
    "from math import exp,log\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import normalize\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import numdifftools as nd\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot, patches\n",
    "from scipy import io\n",
    "import random\n",
    "import scipy.io\n",
    "import os\n",
    "import mat73\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88099be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76020915-7d93-46c0-ac06-f15541d4f9f0",
   "metadata": {},
   "source": [
    "### Accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0468935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: E vector expected communities\n",
    "#        P vector predicted communities\n",
    "def wrong(E,P):\n",
    "    \n",
    "    T = sklearn.metrics.confusion_matrix(E,P) #confusion matrix\n",
    "    \n",
    "    n=0 #counts number of nodes in the wrong community\n",
    "    while T.size != 0:\n",
    "        M = np.max(T)\n",
    "        [x,y] = np.where(T==M)\n",
    "        #use x[0] and y[0] because maybe more entries correspond to maximum value \n",
    "        n += np.sum(T[x[0],:]) - T[x[0],y[0]]\n",
    "        n += np.sum(T[:,y[0]]) - T[x[0],y[0]]\n",
    "        T = np.delete(T, x[0], axis=0)\n",
    "        T = np.delete(T, y[0], axis=1)\n",
    "        \n",
    "    return n\n",
    "\n",
    "#accuracy\n",
    "def accuracy(E,P):\n",
    "    \n",
    "    n = wrong(E,P) #number of nodes in the wrong community\n",
    "    C_exp_len = len(E)\n",
    "    acc = (C_exp_len-wrong(E,P))/C_exp_len\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def accuracy_final(sol_final,C_indexes_known_union,C_exp):\n",
    "    labels_found = np.argmax(sol_final, axis=1)\n",
    "    labels_found = labels_found.transpose()\n",
    "    labels_found = np.delete(labels_found, obj=C_indexes_known_union) #not consider labeled nodes \n",
    "    return accuracy(C_exp,labels_found.A1)\n",
    "\n",
    "def accuracy_Yte(sol_final,labels,index_test):\n",
    "    labels_found_test = np.argmax(sol_final[index_test,:], axis=1)\n",
    "    labels_found_test = labels_found_test.transpose()\n",
    "    return accuracy(labels[index_test],labels_found_test.A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a058587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_adjacency_matrix(G, node_order=None, partitions=[], colors=[]):\n",
    "    \"\"\"\n",
    "    - G is a netorkx graph\n",
    "    - node_order (optional) is a list of nodes, where each node in G\n",
    "          appears exactly once\n",
    "    - partitions is a list of node lists, where each node in G appears\n",
    "          in exactly one node list\n",
    "    - colors is a list of strings indicating what color each\n",
    "          partition should be\n",
    "    If partitions is specified, the same number of colors needs to be\n",
    "    specified.\n",
    "    \"\"\"\n",
    "    adjacency_matrix = nx.to_numpy_matrix(G, dtype=np.bool, nodelist=node_order)\n",
    "\n",
    "    #Plot adjacency matrix in toned-down black and white\n",
    "    fig = pyplot.figure(figsize=(5, 5)) # in inches\n",
    "    pyplot.imshow(adjacency_matrix,\n",
    "                  cmap=\"Greys\",\n",
    "                  interpolation=\"none\")\n",
    "    \n",
    "    # The rest is just if you have sorted nodes by a partition and want to\n",
    "    # highlight the module boundaries\n",
    "    assert len(partitions) == len(colors)\n",
    "    ax = pyplot.gca()\n",
    "    for partition, color in zip(partitions, colors):\n",
    "        current_idx = 0\n",
    "        for module in partition:\n",
    "            ax.add_patch(patches.Rectangle((current_idx, current_idx),\n",
    "                                          len(module), # Width\n",
    "                                          len(module), # Height\n",
    "                                          facecolor=\"none\",\n",
    "                                          edgecolor=color,\n",
    "                                          linewidth=\"1\"))\n",
    "            current_idx += len(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fd9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b, c,r):\n",
    "    assert len(a) == len(b)\n",
    "    np.random.seed(r)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbb73e-8372-461a-b1da-e7cae15ed001",
   "metadata": {},
   "source": [
    "### x_sol - solution lower level problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5146a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_sol(A,N,C,K,lam,theta,y):\n",
    "    \n",
    "    a = theta[K]\n",
    "    if -1e-2<a<0: \n",
    "        a = -1e-2\n",
    "\n",
    "    I = scipy.sparse.identity(N).tocsc()\n",
    "\n",
    "    M_a = scipy.sparse.csr_matrix((N,N))\n",
    "    for k in range(0,K):\n",
    "        M_k = A[k]\n",
    "        M_k.eliminate_zeros()\n",
    "        M_a_k = scipy.sparse.csr_matrix.power(M_k,a)\n",
    "        M_a_k.data[M_a_k.data == np.inf] = 1e+300\n",
    "        M_a_k.data[M_a_k.data == -np.inf] = -1e+300\n",
    "        M_a_k = theta[k]*M_a_k\n",
    "        M_a += M_a_k\n",
    "    M_a.eliminate_zeros()\n",
    "    M_a = scipy.sparse.csr_matrix.power(M_a,1/a)\n",
    "    M_a.data[M_a.data == np.inf] = 1e+300\n",
    "    M_a.data[M_a.data == -np.inf] = -1e+300\n",
    "      \n",
    "    D_a = scipy.sparse.diags(sum(M_a).toarray()[0]).tocsc()\n",
    "    \n",
    "    #solve system using fixed point iterations\n",
    "    B = lam*scipy.sparse.linalg.inv(I + lam*D_a)\n",
    "    BM = B.dot(M_a)\n",
    "    By = B.dot(y)\n",
    "    x_k = scipy.sparse.csr_matrix((N,C))\n",
    "    for i in range(0,100): #10 iterations\n",
    "        x_k = BM.dot(x_k) + By\n",
    "    sol = x_k\n",
    "    \n",
    "    return normalize(sol, norm='l1', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d9d26-b3d1-4917-b4f7-28b8501e7ae4",
   "metadata": {},
   "source": [
    "### ZOFW - Zeroth order Frank Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167bbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frank Wolfe #inexact gradient #Armijo rule \n",
    "sign = lambda x: (1, -1)[x<0]\n",
    "\n",
    "def ZOFW(x_0,fun_theta,K,Y_te,x_sol_theta_dense,tol):\n",
    "    #tol = 1e-4 #tollerance method\n",
    "    max_it = 1e3 #max number iterations\n",
    "    \n",
    "    box_a = [-20,20] #box alpha variable\n",
    "    \n",
    "    #initialization \n",
    "    it = 0\n",
    "    x_it = x_0\n",
    "        \n",
    "    fun_x_it_old = math.inf\n",
    "    fun_x_it = fun_theta(x_it)\n",
    "             \n",
    "    #gradient - Finite-Difference Approximations\n",
    "    c_it = 1e-4\n",
    "    grad_x_it = np.zeros(K+1)\n",
    "    for k in range(0,K+1):\n",
    "        grad_x_it[k] = (fun_theta(x_it+c_it*np.eye(1,K+1,k)[0]) - fun_x_it ) / c_it \n",
    "    grad_a = grad_x_it[-1]\n",
    "    grad_theta = grad_x_it[:-1]   \n",
    "        \n",
    "    #minimize linearize function\n",
    "    #beta #unit simplex #vector of the canonical basis corrispondent to min index gradient \n",
    "    x_it_theta = np.eye(1,K,np.argmin(grad_theta))\n",
    "    #alpha #box #sign\n",
    "    if grad_a>0:\n",
    "        x_it_alpha = box_a[0]\n",
    "    else:\n",
    "        x_it_alpha = box_a[1]\n",
    "    #argmin  \n",
    "    x_it_min = np.append(x_it_theta,x_it_alpha)\n",
    "        \n",
    "    #descent direction\n",
    "    d_it = x_it_min - x_it\n",
    "    \n",
    "    while it<max_it and grad_x_it.dot(d_it) <= -tol and fun_x_it_old-fun_x_it>1e-6:\n",
    "\n",
    "        it += 1\n",
    "        fun_x_it_old = fun_x_it \n",
    "             \n",
    "        #step size #Armijo rule\n",
    "        step = 1\n",
    "        x_it_d = x_it + step*d_it\n",
    "        fun_x_it_d = fun_theta(x_it_d)       \n",
    "        while fun_x_it_d > fun_x_it + 0.1*step*(d_it.dot(grad_x_it)):\n",
    "            step = step/2\n",
    "            x_it_d = x_it + step*d_it\n",
    "            fun_x_it_d = fun_theta(x_it_d)  \n",
    "        else:\n",
    "            x_it = x_it_d\n",
    "        \n",
    "        fun_x_it = fun_theta(x_it)                 \n",
    "    \n",
    "        #gradient - Finite-Difference Approximations\n",
    "        c_it = c_it/2\n",
    "        grad_x_it = np.zeros(K+1)\n",
    "        for k in range(0,K+1):\n",
    "            grad_x_it[k] = (fun_theta(x_it+c_it*np.eye(1,K+1,k)[0]) - fun_x_it ) / c_it \n",
    "        grad_a = grad_x_it[-1]\n",
    "        grad_theta = grad_x_it[:-1] \n",
    "        \n",
    "        #minimize linearize function\n",
    "        #beta #unit simplex #vector of the canonical basis corrispondent to min index gradient \n",
    "        x_it_theta = np.eye(1,K,np.argmin(grad_theta))\n",
    "        #alpha #box #sign\n",
    "        if grad_a>0:\n",
    "            x_it_alpha = box_a[0]\n",
    "        else:\n",
    "            x_it_alpha = box_a[1]\n",
    "        #argmin  \n",
    "        x_it_min = np.append(x_it_theta,x_it_alpha)\n",
    "    \n",
    "        #descent direction\n",
    "        d_it = x_it_min - x_it \n",
    "        \n",
    "    return x_it,it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fa31b-1265-4df7-86b0-a35cc98baf1a",
   "metadata": {},
   "source": [
    "### Cross_entropy - loss function upper level problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(N,C,x,x_a):\n",
    "    \n",
    "    x = np.transpose(x.toarray())\n",
    "    x_a = x_a.toarray()\n",
    "    \n",
    "    x_a_ = np.copy(x_a)\n",
    "    x_a_[x_a_<=1e-15] = 1e-15 #otherwise log(0)=-inf\n",
    "    log_x_a = np.log(x_a_)\n",
    "    x_log_x_a = np.matmul(x,log_x_a).item()\n",
    "    \n",
    "    x_a_ = 1 - np.copy(x_a)\n",
    "    x_a_[x_a_<=1e-15] = 1e-15 #otherwise log(0)=-inf\n",
    "    log_1_x_a = np.log(x_a_)\n",
    "    x_log_1_x_a = np.matmul(1-x,log_1_x_a).item()\n",
    "    \n",
    "    val = -(1/N) * (x_log_x_a + x_log_1_x_a)\n",
    "    return val\n",
    "\n",
    "\n",
    "def cross_entropy_multiclass(N,C,x,x_a):\n",
    "\n",
    "    num = x_a.toarray()\n",
    "    den = scipy.sparse.csr_matrix.sum(x_a,axis=1)\n",
    "    den[den==0] = 1 #can not divide by zero \n",
    "    frac = num/den\n",
    "    frac[frac<=1e-15] = 1e-15 #otherwise log(0)=-inf   \n",
    "    #log\n",
    "    frac_log = np.log(frac)\n",
    "    \n",
    "    #cross-entropy\n",
    "    val = -(1/N) * (np.multiply(x.toarray(),frac_log)).sum() \n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c2a1b-7dba-4dff-a7e7-216739f9d0e8",
   "metadata": {},
   "source": [
    "### Parallelization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d50c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_c(A,N,K,C,lam,Y_tr,Y_te,Y,x_0,cross_entropy,c):\n",
    "    x_sol_theta = lambda theta: x_sol(A,N,1,K,lam,theta,Y_tr[:,c]) \n",
    "    fun_theta = lambda theta: cross_entropy(N,C,Y_te[:,c],x_sol_theta(theta))\n",
    "    theta,it_c = ZOFW(x_0,fun_theta,K,Y_te[:,c],x_sol_theta,1e-4)\n",
    "\n",
    "    #recalculate solution using selected alpha and all known labels\n",
    "    x_final = x_sol(A,N,1,K,lam,theta,Y[:,c])\n",
    "    loss = cross_entropy(N,C,Y[:,c],x_final)\n",
    "    \n",
    "    return x_final,theta,loss\n",
    "\n",
    "def multistart(A,N,K,C,lam,Y_tr,Y_te,Y,cross_entropy,cross_entropy_c,list_x_0,rrrr):\n",
    "    #initialization FW\n",
    "    x_0 = list_x_0[:,rrrr]\n",
    "    \n",
    "    x_final_c,theta_c,loss_c = zip(*Parallel(n_jobs=2)(delayed(cross_entropy_c)(A,N,K,C,lam,Y_tr,Y_te,Y,x_0,cross_entropy,c) for c in range(C)))\n",
    "    sol_final = scipy.sparse.hstack(x_final_c)\n",
    "    theta = np.transpose(np.asarray(theta_c))\n",
    "    loss = sum(loss_c)\n",
    "    \n",
    "    return sol_final,theta,loss\n",
    "\n",
    "def multistart_multi(A,N,K,C,lam,theta,Y_tr,Y_te,Y,cross_entropy_multi,fun_theta,x_sol_theta,list_x_0,rrrr):\n",
    "    \n",
    "    #initialization FW\n",
    "    x_0 = list_x_0[:,rrrr]\n",
    "    theta,it = ZOFW(x_0,fun_theta,K,Y_te,x_sol_theta,1e-8)\n",
    "    \n",
    "    #recalculate solution using selected alpha and all known labels           \n",
    "    sol_final = x_sol(A,N,C,K,lam,theta,Y)\n",
    "    loss = cross_entropy_multi(N,C,Y,sol_final)\n",
    "    \n",
    "    return sol_final,theta,loss\n",
    "\n",
    "\n",
    "def methods(A,N,K,C,lam,Y,labels_known_size,perc_,labels,C_indexes_known,sample2,rrr):\n",
    "    \n",
    "    labels_test_size = np.rint(labels_known_size*(perc_/100),out=np.zeros(C,int),casting='unsafe')\n",
    "    labels_train_size = labels_known_size - labels_test_size\n",
    "    \n",
    "    index_train = []\n",
    "    index_test = []\n",
    "    for c in range(0,C):   \n",
    "        shift = int(labels_train_size[c]/(2*sample2))\n",
    "        index_train_c = C_indexes_known[c][rrr*shift:labels_train_size[c]+rrr*shift] #each sample shift windows \n",
    "        index_train.append(index_train_c)\n",
    "        index_test.append(np.setdiff1d(C_indexes_known[c],index_train_c))\n",
    "    index_train = np.concatenate(index_train)\n",
    "    index_test = np.concatenate(index_test)\n",
    "    \n",
    "    #matrices Y_tr and Y_te #corrispondent to labels training and validation sets\n",
    "    row = index_train\n",
    "    comm_list_train = []\n",
    "    for c in range(0,C):\n",
    "        comm_list_train.append([c]*labels_train_size[c])\n",
    "    comm_list_train = np.concatenate(comm_list_train)\n",
    "    col = np.array(comm_list_train)\n",
    "    data = np.array(np.ones(sum(labels_train_size)))\n",
    "    Y_tr = scipy.sparse.csr_matrix((data, (row, col)), shape=(N, C))\n",
    "    # normalized\n",
    "    Y_tr = normalize(Y_tr, norm='l1', axis=0)\n",
    "\n",
    "    row = index_test\n",
    "    comm_list_test = []\n",
    "    for c in range(0,C):\n",
    "        comm_list_test.append([c]*labels_test_size[c])\n",
    "    comm_list_test = np.concatenate(comm_list_test)\n",
    "    col = np.array(comm_list_test)\n",
    "    data = np.array(np.ones(sum(labels_test_size)))\n",
    "    Y_te = scipy.sparse.csr_matrix((data, (row, col)), shape=(N, C))\n",
    "    # normalized\n",
    "    Y_te = normalize(Y_te, norm='l1', axis=0)\n",
    "\n",
    "    #multi-start #list random initial points FW\n",
    "    random.seed(rrr)\n",
    "    np.random.seed(rrr)\n",
    "    multistart_n = 10\n",
    "    list_x_0 = np.zeros((K+1,multistart_n))\n",
    "    for rrrr in range(0,multistart_n):\n",
    "        list_x_0[:,rrrr] = np.append(np.random.dirichlet(np.ones(K),size=1),random.uniform(-20,20))\n",
    "    #last two initial points fixed [1/K,..,1/k,1], [1/K,..,1/k,-1]\n",
    "    u = np.ones(K+1) / K #theta sum 1 \n",
    "    u[K] = 1 #alpha\n",
    "    w = u.copy()\n",
    "    w[K] = -1\n",
    "    list_x_0[:,0] = u\n",
    "    list_x_0[:,1] = w\n",
    "\n",
    "\n",
    "    #loss function: binomial classification, each class separate\n",
    "    #save value function each point #select minimum and corrispondent solution and variables \n",
    "    sol_final_multistart,theta_multistart,loss_multistart = zip(*Parallel(n_jobs=2,prefer=\"threads\")(delayed(multistart)(A,N,K,C,lam,Y_tr,Y_te,Y,cross_entropy,cross_entropy_c,list_x_0,rrrr) for rrrr in range(multistart_n)))\n",
    "\n",
    "    #choose sol corrispondent to min vaue loss \n",
    "    index_min = loss_multistart.index(min(loss_multistart))\n",
    "    sol_final = sol_final_multistart[index_min]\n",
    "    theta = theta_multistart[index_min]\n",
    " \n",
    "    #calculate accuracy just over Y_tr, in order to choose better sample Y_tr from Y \n",
    "    acc_k = accuracy_Yte(sol_final,labels,index_test)\n",
    "    sol_final_0 = sol_final\n",
    "    matrix_acc_0 = acc_k\n",
    "    matrix_theta_0 = theta\n",
    "\n",
    "    #loss function: multiclass cross-entropy\n",
    "    x_sol_theta = lambda theta: x_sol(A,N,C,K,lam,theta,Y_tr)\n",
    "    fun_theta = lambda theta: cross_entropy_multiclass(N,C,Y_te,x_sol_theta(theta))\n",
    "    sol_final_multistart,theta_multistart,loss_multistart = zip(*Parallel(n_jobs=2,prefer=\"threads\")(delayed(multistart_multi)(A,N,K,C,lam,theta,Y_tr,Y_te,Y,cross_entropy_multiclass,fun_theta,x_sol_theta,list_x_0,rrrr) for rrrr in range(multistart_n)))\n",
    "\n",
    "    #choose sol corrispondent to min vaue loss \n",
    "    index_min = loss_multistart.index(min(loss_multistart))\n",
    "    sol_final = sol_final_multistart[index_min]\n",
    "    theta = theta_multistart[index_min]\n",
    "\n",
    "    #calculate accuracy just over Y_tr, in order to choose better sample Y_tr from Y \n",
    "    acc_k = accuracy_Yte(sol_final,labels,index_test)\n",
    "    sol_final_1 = sol_final\n",
    "    matrix_acc_1 = acc_k\n",
    "    matrix_theta_1 = np.array([theta]*C).transpose()\n",
    "    \n",
    "    return matrix_acc_0,sol_final_0,matrix_theta_0,matrix_acc_1,sol_final_1,matrix_theta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0523b-a451-497e-a239-3db3e77db51c",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58239c43-229d-4ff5-b5ab-ef3810292cd5",
   "metadata": {},
   "source": [
    "#### synthetic_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cbf960-3ca4-49f6-a731-d9ca52da6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_datasets(N,C,K,case,cluster_std,knn,perc,lam):\n",
    "    \n",
    "    #random samples\n",
    "    sample1 = 5 #5 #random matrices A\n",
    "    sample2 = 5 #5 #division training and test sets Y_tr Y_te #just for proposed methods \n",
    "\n",
    "    #list accuracy methods #for each random marix A \n",
    "    matrix_acc = np.zeros((K+5+2,sample1))  \n",
    "    #list values theta and alpha porposed methods \n",
    "    matrix_theta = np.zeros((sample1,2,K+1,C))\n",
    "\n",
    "    #save for state-of the-art-methods in Matlab\n",
    "    A_save = []\n",
    "    labels_save = []\n",
    "    Y_save = []\n",
    "    for r in tqdm(range(0,sample1)): #random matrices\n",
    "\n",
    "        #generate weigthed random networks \n",
    "        n = 5 #space dimention\n",
    "        random_state = r #random sample\n",
    "        X, labels, centers = make_blobs(n_samples=N, centers=C, n_features=n, random_state=random_state,cluster_std=cluster_std, return_centers=True)\n",
    "        #plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "\n",
    "        #rename labels from 0 \n",
    "        labels_unique,labels = np.unique(labels, return_inverse=True)  \n",
    "        # communties size\n",
    "        C_size = np.zeros(C)\n",
    "        for com in range(0,C):\n",
    "            com_ind = np.where(labels == com)\n",
    "            C_size[com] = len(com_ind[0])\n",
    "\n",
    "        #generate correspondent affinity matrices\n",
    "        A = [] #list #A[k] sparse adjacency matrix layer k\n",
    "\n",
    "        if case=='info2': #informative case #different nodes\n",
    "            if K>1:\n",
    "                #1 layer: Euclidean distance\n",
    "                W = kneighbors_graph(X, n_neighbors=knn, mode='distance', include_self=False)\n",
    "                W = 0.5 * (W + W.T)\n",
    "                index_com = np.where(labels == 0)\n",
    "                for com in range(1,C): \n",
    "                    index_com = np.concatenate((index_com, np.where(labels == com)), axis=None)\n",
    "                W = W[index_com, :]\n",
    "                W = W[:, index_com]\n",
    "                A.append(W)\n",
    "                #others layers random #new generated network #Euclidean distance\n",
    "                for k in range(1,K):\n",
    "                    X_, labels_, centers_ = make_blobs(n_samples=N, centers=C, n_features=n, random_state=r+random_state+k,cluster_std=cluster_std, return_centers=True)\n",
    "                    W_ = kneighbors_graph(X_, n_neighbors=knn, mode='distance', include_self=False)\n",
    "                    W_ = 0.5 * (W_ + W_.T)\n",
    "                    #rename labels from 0 \n",
    "                    labels_unique_,labels_ = np.unique(labels_, return_inverse=True)  \n",
    "                    # communties size\n",
    "                    index_com_ = np.where(labels_ == 0)\n",
    "                    for com in range(1,C): \n",
    "                        index_com_ = np.concatenate((index_com_, np.where(labels_ == com)), axis=None)\n",
    "                    W_ = W_[index_com_, :]\n",
    "                    W_ = W_[:, index_com_]                   \n",
    "                    A.append(W_)\n",
    "\n",
    "\n",
    "        if case=='noisy': #noisy case \n",
    "            if K>1:\n",
    "                #1 layer: Euclidean distance\n",
    "                W = kneighbors_graph(X, n_neighbors=knn, mode='distance', include_self=False)\n",
    "                W = 0.5 * (W + W.T)\n",
    "                A.append(W)\n",
    "                #2 layer: random \n",
    "                #others layers random \n",
    "                for k in range(1,K):\n",
    "                    X_, labels_, centers_ = make_blobs(n_samples=N, centers=C, n_features=n, random_state=r+random_state+k,cluster_std=cluster_std, return_centers=True)\n",
    "                    W_ = kneighbors_graph(X_, n_neighbors=knn, mode='distance', include_self=False)\n",
    "                    W_ = 0.5 * (W_ + W_.T)\n",
    "                    index = np.arange(N)\n",
    "                    np.random.shuffle(index)\n",
    "                    W_ = W_[index, :]\n",
    "                    W_ = W_[:, index]\n",
    "                    A.append(W_)\n",
    "\n",
    "        if case=='compl': #complementary case #each layer one community \n",
    "            W_list = []\n",
    "            #1 layer eucledian #info \n",
    "            W = kneighbors_graph(X, n_neighbors=knn, mode='distance', include_self=False)\n",
    "            W = 0.5 * (W + W.T)\n",
    "            A.append(W)\n",
    "            #others layers random #less connections\n",
    "            knn_ = int(knn/3)    \n",
    "            for k in range(1,K+1):\n",
    "                X_, labels_, centers_ = make_blobs(n_samples=N, centers=C, n_features=n, random_state=r+random_state+k,cluster_std=cluster_std, return_centers=True)\n",
    "                W_ = kneighbors_graph(X_, n_neighbors=knn_, mode='distance', include_self=False)\n",
    "                W_ = 0.5 * (W_ + W_.T)\n",
    "                index = np.arange(N)\n",
    "                np.random.shuffle(index)\n",
    "                W_ = W_[index, :]\n",
    "                W_ = W_[:, index]\n",
    "                A.append(W_)\n",
    "\n",
    "        if case!='info2':\n",
    "            #reorder A adjacency matrix by communities\n",
    "            index_com = np.where(labels == 0)\n",
    "            for com in range(1,C): \n",
    "                index_com = np.concatenate((index_com, np.where(labels == com)), axis=None)\n",
    "            for k in range(0,K):\n",
    "                A[k] = A[k][index_com, :]\n",
    "                A[k] = A[k][:, index_com]\n",
    "                \n",
    "        labels = []\n",
    "        for com in range(0,C):\n",
    "            labels = np.concatenate((labels, [com]*int(C_size[com])), axis=None)\n",
    "\n",
    "        # communities indeces \n",
    "        C_indexes = np.zeros(C, dtype=object)\n",
    "        for com in range(0,C):\n",
    "            com_ind = np.where(labels == com)\n",
    "            C_indexes[com] = com_ind\n",
    "\n",
    "        # for each community a percentage per of labels are known\n",
    "        labels_known_size = np.rint(C_size*(perc/100),out=np.zeros(C,int),casting='unsafe') #number of known labels in each community\n",
    "        labels_known_size[np.where(labels_known_size < 2)] = 2 #at least 2 node labeled each community\n",
    "        labels_known_size[np.where(C_size == 1)] = 1 #unless community just one node\n",
    "\n",
    "        if case=='compl':\n",
    "            #one community each layer\n",
    "            A_copy = []\n",
    "            for k in range(0,K):\n",
    "                A_copy.append(A[k].copy())\n",
    "            sum_com = 0\n",
    "            for k in range(0,K):\n",
    "                sum_com_old = sum_com\n",
    "                sum_com += int(C_size[k])\n",
    "                A[k+1][sum_com_old:sum_com, sum_com_old:sum_com] = A_copy[0][sum_com_old:sum_com, sum_com_old:sum_com]\n",
    "            del A[0]\n",
    "\n",
    "        #normalize matrices \n",
    "        for k in range(0,K):\n",
    "            A[k] = A[k]/A[k].max()\n",
    "\n",
    "        A_save.append(A)\n",
    "        labels_save.append(labels)\n",
    "\n",
    "#         #plot communities\n",
    "#         for k in range(0,K):\n",
    "#             G = nx.from_scipy_sparse_matrix(A[k])\n",
    "#             draw_adjacency_matrix(G) \n",
    "\n",
    "        #Y known labels\n",
    "        np.random.seed(0) \n",
    "        C_indexes_known = np.zeros(C, dtype=object)\n",
    "        for com in range(0,C):\n",
    "            C_indexes_known[com] = np.random.choice(C_indexes[com][0],labels_known_size[com],replace=False)\n",
    "\n",
    "        # Y matrix #known labels\n",
    "        C_indexes_known_union = np.concatenate(C_indexes_known)\n",
    "        comm_list = []\n",
    "        for c in range(0,C):\n",
    "            comm_list.append([c]*labels_known_size[c])\n",
    "        comm_list = np.concatenate(comm_list)\n",
    "        row = C_indexes_known_union\n",
    "        col = np.array(comm_list)\n",
    "        data = np.array(np.ones(sum(labels_known_size)))\n",
    "        Y = scipy.sparse.csr_matrix((data, (row, col)), shape=(N, C))\n",
    "        # normalized\n",
    "        Y = normalize(Y, norm='l2', axis=0) #check scipy.sparse.linalg.norm(Y, axis=0)\n",
    "\n",
    "        # expected communities (without known labels) #to calculate final accuracy\n",
    "        C_exp = np.delete(labels, obj=C_indexes_known_union)\n",
    "\n",
    "        Y_save.append(Y)\n",
    "\n",
    "        #acc_single_layers\n",
    "        for k in range(0,K):\n",
    "            theta = np.ones((1,2))[0]\n",
    "            sol_final = x_sol([A[k]],N,C,1,lam,theta,Y)\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[k,r] = acc_k\n",
    "            \n",
    "        #alpha=1 #arithmetic mean     \n",
    "        a = 1 #alpha 1\n",
    "        theta = np.ones(K+1) / K #theta sum 1 \n",
    "        theta[-1] = 1 #alpha 1\n",
    "        sol_final = x_sol(A,N,C,K,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K,r]  = acc_k\n",
    "\n",
    "        #alpha=-1 #harmonic mean    \n",
    "        sol_final = scipy.sparse.lil_matrix((N,C))\n",
    "        theta = np.ones(K+1) / K #theta sum 1 \n",
    "        theta[-1] = -1 #alpha -1\n",
    "        sol_final = x_sol(A,N,C,K,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+1,r]  = acc_k\n",
    "\n",
    "        #alpha=+inf #maximum    \n",
    "        A_max = A[0]\n",
    "        for k in range(0,K-1):\n",
    "            A_max = A_max.maximum(A[k+1])\n",
    "        theta = np.ones((1,2))[0]\n",
    "        sol_final = x_sol([A_max],N,C,1,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+2,r]  = acc_k\n",
    "\n",
    "        #alpha=-inf #minimum    \n",
    "        A_min = A[0]\n",
    "        for k in range(0,K-1):\n",
    "            A_min = A_min.minimum(A[k+1])\n",
    "        theta = np.ones((1,2))[0]\n",
    "        sol_final = x_sol([A_min],N,C,1,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+3,r]  = acc_k\n",
    "\n",
    "        #alpha=0 #geometric mean    \n",
    "        A_prod = A[0]\n",
    "        for k in range(0,K-1):\n",
    "            A_prod = A_prod.multiply(A[k+1])\n",
    "        A_prod = scipy.sparse.csr_matrix.power(A_prod,1/K)\n",
    "        theta = np.ones((1,2))[0]\n",
    "        sol_final = x_sol([A_prod],N,C,1,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+4,r]  = acc_k \n",
    "\n",
    "        #proposed methods #parallelize sampling training and test sets\n",
    "        perc_ = 80 #percentage test set\n",
    "        matrix_acc_0,sol_final_0,matrix_theta_0,matrix_acc_1,sol_final_1,matrix_theta_1 = zip(*Parallel(n_jobs=2,prefer=\"threads\")(delayed(methods)(A,N,K,C,lam,Y,labels_known_size,perc_,labels,C_indexes_known,sample2,rrr) for rrr in range(sample2)))\n",
    "\n",
    "        argmax0 = np.argmax(matrix_acc_0)\n",
    "        sol_final = sol_final_0[argmax0]\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+5,r]  = acc_k\n",
    "        matrix_theta[r,0,:,:]=matrix_theta_0[argmax0]\n",
    "\n",
    "        argmax1 = np.argmax(matrix_acc_1)\n",
    "        sol_final = sol_final_1[argmax1]\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+6,r]  = acc_k\n",
    "        matrix_theta[r,1,:,:]=matrix_theta_1[argmax1]\n",
    "                \n",
    "                \n",
    "    #save matrix results samples\n",
    "    with open('Results/matrix_acc_theta_N'+str(N)+'_C'+str(C)+'_case '+case+'_std'+str(cluster_std)+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_sample'+str(sample1),\"wb\") as fp:\n",
    "        pickle.dump(matrix_acc,fp)\n",
    "    with open('Results/matrix_theta_theta_N'+str(N)+'_C'+str(C)+'_case '+case+'_std'+str(cluster_std)+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_sample'+str(sample1),\"wb\") as fp:\n",
    "        pickle.dump(matrix_theta,fp)\n",
    "        \n",
    "    #save for state-of the-art-methods in Matlab\n",
    "    my_file = 'Results/Datasets_Matlab/Matlab_theta_N'+str(N)+'_C'+str(C)+'_case '+case+'_std'+str(cluster_std)+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_sample'+str(sample1)+\".mat\"\n",
    "    scipy.io.savemat(my_file, dict(A_list=A_save, Y_list=Y_save, labels_list=labels_save))\n",
    "    \n",
    "    return A_save,labels_save,Y_save,matrix_acc,matrix_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc9aa4-6778-4abe-8f2f-7e936ec966cf",
   "metadata": {},
   "source": [
    "#### real_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6119f4-9dbb-49a5-a27a-555a6d84ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_datasets(datasets_list):\n",
    "    for dataset in datasets_list: \n",
    "        my_path = 'Datasets'   \n",
    "        my_file = dataset+\".mat\"\n",
    "        file_path = os.path.join(my_path, my_file)\n",
    "        data = scipy.io.loadmat(file_path)\n",
    "        X = data['W_cell']\n",
    "        if dataset in [\"UCI_mfeat\",\"cora\",\"citeseer\"]:\n",
    "            labels = data['labels'].toarray().transpose()[0]\n",
    "        else:\n",
    "            labels = data['labels'].toarray()[0]\n",
    "        #information \n",
    "        N = len(labels) #number of nodes\n",
    "        #rename labels from 0 \n",
    "        labels_unique,labels = np.unique(labels, return_inverse=True) \n",
    "        C = len(labels_unique) #number of communities \n",
    "        K = len(X) #number layers\n",
    "        print(dataset + ' - nodes: ' + str(N) + ', communities: ' + str(C) + ', layers: ' + str(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08446b68-1883-4189-ab78-4ceccec88419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_datasets(dataset,knn,perc,lam): \n",
    "    \n",
    "    #random samples\n",
    "    sample1 = 5 #known labels Y\n",
    "    sample2 = 5 #division training and test sets Y_tr Y_te #just for proposed methods \n",
    "    \n",
    "    #download matrix \n",
    "    my_path = 'Datasets'   \n",
    "    my_file = dataset+\".mat\"\n",
    "    file_path = os.path.join(my_path, my_file)\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    X = data['W_cell']\n",
    "    if dataset in [\"UCI_mfeat\",\"cora\",\"citeseer\"]:\n",
    "        labels = data['labels'].toarray().transpose()[0]\n",
    "    else:\n",
    "        labels = data['labels'].toarray()[0]\n",
    "    #information \n",
    "    N = len(labels) #number of nodes\n",
    "    #rename labels from 0 \n",
    "    labels_unique,labels = np.unique(labels, return_inverse=True) \n",
    "    C = len(labels_unique) #number of communities \n",
    "    K = len(X) #number layers\n",
    "    \n",
    "    #list accuracy methods #for each random marix A \n",
    "    matrix_acc = np.zeros((K+5+2,sample1))  \n",
    "    #list values theta and alpha porposed methods \n",
    "    matrix_theta = np.zeros((sample1,2,K+1,C))\n",
    "    \n",
    "    # communities indeces # communties size\n",
    "    C_size = np.zeros(C)\n",
    "    for com in range(0,C):\n",
    "        com_ind = np.where(labels == com)\n",
    "        #C_indexes[com] = com_ind\n",
    "        C_size[com] = len(com_ind[0])\n",
    "\n",
    "    #reorder A adjacency matrix by communities\n",
    "    index_com = np.where(labels == 0)\n",
    "    for com in range(1,C): \n",
    "        index_com = np.concatenate((index_com, np.where(labels == com)), axis=None)\n",
    "\n",
    "    A = list(X)\n",
    "    for k in range(0,K):\n",
    "        A[k] = A[k][0]\n",
    "\n",
    "    for k in range(0,K):\n",
    "        A[k] = A[k][index_com, :]\n",
    "        A[k] = A[k][:, index_com]\n",
    "\n",
    "    labels = []\n",
    "    for com in range(0,C):\n",
    "        labels = np.concatenate((labels, [com]*int(C_size[com])), axis=None)\n",
    "\n",
    "    # communities indeces # communties size\n",
    "    C_indexes = np.zeros(C, dtype=object)\n",
    "    for com in range(0,C):\n",
    "        com_ind = np.where(labels == com)\n",
    "        C_indexes[com] = com_ind\n",
    "\n",
    "    # for each community a percentage of labels are known\n",
    "    #perc = 10 #percentage known labels each community \n",
    "    labels_known_size = np.rint(C_size*(perc/100),out=np.zeros(C,int),casting='unsafe') #number of known labels in each community\n",
    "    labels_known_size[np.where(labels_known_size < 2)] = 2 #at least 2 node labeled each community\n",
    "    labels_known_size[np.where(C_size == 1)] = 1 #unless community just one node\n",
    "\n",
    "    # #normalize matrices \n",
    "    # for k in range(0,K):\n",
    "    #     A[k] = A[k]/A[k].max()\n",
    "\n",
    "    # for k in range(0,K):\n",
    "    #     G = nx.from_scipy_sparse_matrix(A[k])\n",
    "    #     draw_adjacency_matrix(G)\n",
    "\n",
    "    Y_save = [] #save for state-of the-art-methods in Matlab\n",
    "    for r in tqdm(range(0,sample1)): #random indices know labels Y\n",
    "        \n",
    "        # indices know labels each community \n",
    "        np.random.seed(r) \n",
    "        C_indexes_known = np.zeros(C, dtype=object)\n",
    "        for com in range(0,C):\n",
    "            C_indexes_known[com] = np.random.choice(C_indexes[com][0],labels_known_size[com],replace=False)\n",
    "  \n",
    "        # Y matrix #known labels\n",
    "        C_indexes_known_union = np.concatenate(C_indexes_known)\n",
    "        comm_list = []\n",
    "        for c in range(0,C):\n",
    "            comm_list.append([c]*labels_known_size[c])\n",
    "        comm_list = np.concatenate(comm_list)\n",
    "        row = C_indexes_known_union\n",
    "        col = np.array(comm_list)\n",
    "        data = np.array(np.ones(sum(labels_known_size)))\n",
    "        Y = scipy.sparse.csr_matrix((data, (row, col)), shape=(N, C))\n",
    "        # normalized\n",
    "        Y = normalize(Y, norm='l2', axis=0) #check scipy.sparse.linalg.norm(Y, axis=0)\n",
    "        \n",
    "        # expected communities (without known labels) #to calculate final accuracy\n",
    "        C_exp = np.delete(labels, obj=C_indexes_known_union)\n",
    "\n",
    "        Y_save.append(Y)\n",
    "        \n",
    "        #acc_single_layers\n",
    "        for k in range(0,K):\n",
    "            theta = np.ones((1,2))[0]\n",
    "            sol_final = x_sol([A[k]],N,C,1,lam,theta,Y)\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[k,r] = acc_k\n",
    "            \n",
    "        #alpha=1 #arithmetic mean     \n",
    "        a = 1 #alpha 1\n",
    "        theta = np.ones(K+1) / K #theta sum 1 \n",
    "        theta[-1] = 1 #alpha 1\n",
    "        sol_final = x_sol(A,N,C,K,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K,r]  = acc_k\n",
    "\n",
    "        #alpha=-1 #harmonic mean    \n",
    "        sol_final = scipy.sparse.lil_matrix((N,C))\n",
    "        theta = np.ones(K+1) / K #theta sum 1 \n",
    "        theta[-1] = -1 #alpha -1\n",
    "        sol_final = x_sol(A,N,C,K,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+1,r]  = acc_k\n",
    "\n",
    "        #alpha=+inf #maximum    \n",
    "        A_max = A[0]\n",
    "        for k in range(0,K-1):\n",
    "            A_max = A_max.maximum(A[k+1])\n",
    "        theta = np.ones((1,2))[0]\n",
    "        sol_final = x_sol([A_max],N,C,1,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+2,r]  = acc_k\n",
    "\n",
    "        #alpha=-inf #minimum    \n",
    "        A_min = A[0]\n",
    "        for k in range(0,K-1):\n",
    "            A_min = A_min.minimum(A[k+1])\n",
    "        theta = np.ones((1,2))[0]\n",
    "        sol_final = x_sol([A_min],N,C,1,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+3,r]  = acc_k\n",
    "\n",
    "        #alpha=0 #geometric mean    \n",
    "        A_prod = A[0]\n",
    "        for k in range(0,K-1):\n",
    "            A_prod = A_prod.multiply(A[k+1])\n",
    "        A_prod = scipy.sparse.csr_matrix.power(A_prod,1/K)\n",
    "        theta = np.ones((1,2))[0]\n",
    "        sol_final = x_sol([A_prod],N,C,1,lam,theta,Y)\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+4,r]  = acc_k \n",
    "\n",
    "        #proposed methods #parallelize sampling training and test sets\n",
    "        perc_ = 80 #perc_ = 50 #percentage test set\n",
    "        matrix_acc_0,sol_final_0,matrix_theta_0,matrix_acc_1,sol_final_1,matrix_theta_1 = zip(*Parallel(n_jobs=2,prefer=\"threads\")(delayed(methods)(A,N,K,C,lam,Y,labels_known_size,perc_,labels,C_indexes_known,sample2,rrr) for rrr in range(sample2)))\n",
    "\n",
    "        argmax0 = np.argmax(matrix_acc_0)\n",
    "        sol_final = sol_final_0[argmax0]\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+5,r]  = acc_k\n",
    "        matrix_theta[r,0,:,:]=matrix_theta_0[argmax0]\n",
    "\n",
    "        argmax1 = np.argmax(matrix_acc_1)\n",
    "        sol_final = sol_final_1[argmax1]\n",
    "        acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "        matrix_acc[K+6,r]  = acc_k\n",
    "        matrix_theta[r,1,:,:]=matrix_theta_1[argmax1]\n",
    "                \n",
    "    #save matrix results samples\n",
    "    with open(\"Results_Real/Matlab_acc_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam),\"wb\") as fp:\n",
    "        pickle.dump(matrix_acc,fp)\n",
    "    with open(\"Results_Real/Matlab_theta_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam),\"wb\") as fp:\n",
    "        pickle.dump(matrix_theta,fp)\n",
    "    #save for state-of the-art-methods in Matlab \n",
    "    my_file = \"Results_Real/Datasets_Matlab_Real/Matlab_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+\".mat\"\n",
    "    scipy.io.savemat(my_file, dict(A_list=A, Y_list=Y_save, labels_list=labels))\n",
    "\n",
    "    return A,labels,Y_save,matrix_acc,matrix_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae9321-ece4-4cef-a0e7-0513d34b190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_datasets_noisy(dataset,knn,perc,lam,rand_layers): \n",
    "    \n",
    "    #random samples\n",
    "    sample1 = 1 #known labels Y\n",
    "    sample2 = 5 #division training and test sets Y_tr Y_te #just for proposed methods\n",
    "    sample3 = 3 #random layers added \n",
    "       \n",
    "    #download matrix \n",
    "    my_path = 'Datasets'   \n",
    "    my_file = dataset+\".mat\"\n",
    "    file_path = os.path.join(my_path, my_file)\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    X = data['W_cell']\n",
    "    if dataset in [\"UCI_mfeat\",\"cora\",\"citeseer\"]:\n",
    "        labels = data['labels'].toarray().transpose()[0]\n",
    "    else:\n",
    "        labels = data['labels'].toarray()[0]\n",
    "    #information \n",
    "    N = len(labels) #number of nodes\n",
    "    #rename labels from 0 \n",
    "    labels_unique,labels = np.unique(labels, return_inverse=True) \n",
    "    C = len(labels_unique) #number of communities \n",
    "    K = len(X) #number layers\n",
    "\n",
    "    # communities indeces # communties size\n",
    "    #C_indexes = np.zeros(C, dtype=object)\n",
    "    C_size = np.zeros(C)\n",
    "    for com in range(0,C):\n",
    "        com_ind = np.where(labels == com)\n",
    "        #C_indexes[com] = com_ind\n",
    "        C_size[com] = len(com_ind[0])\n",
    "\n",
    "    #reorder A adjacency matrix by communities\n",
    "    index_com = np.where(labels == 0)\n",
    "    for com in range(1,C): \n",
    "        index_com = np.concatenate((index_com, np.where(labels == com)), axis=None)\n",
    "\n",
    "    A_ = list(X)\n",
    "    for k in range(0,K):\n",
    "        A_[k] = A_[k][0]\n",
    "\n",
    "    for k in range(0,K):\n",
    "        A_[k] = A_[k][index_com, :]\n",
    "        A_[k] = A_[k][:, index_com]\n",
    "\n",
    "    labels = []\n",
    "    for com in range(0,C):\n",
    "        labels = np.concatenate((labels, [com]*int(C_size[com])), axis=None)\n",
    "\n",
    "    # communities indeces # communties size\n",
    "    C_indexes = np.zeros(C, dtype=object)\n",
    "    for com in range(0,C):\n",
    "        com_ind = np.where(labels == com)\n",
    "        C_indexes[com] = com_ind\n",
    "\n",
    "    # for each community a percentage of labels are known\n",
    "    #perc = 10 #percentage known labels each community \n",
    "    labels_known_size = np.rint(C_size*(perc/100),out=np.zeros(C,int),casting='unsafe') #number of known labels in each community\n",
    "    labels_known_size[np.where(labels_known_size < 2)] = 2 #at least 2 node labeled each community\n",
    "    labels_known_size[np.where(C_size == 1)] = 1 #unless community just one node\n",
    "\n",
    "    # for k in range(0,K):\n",
    "    #     G = nx.from_scipy_sparse_matrix(A_[k])\n",
    "    #     draw_adjacency_matrix(G)\n",
    "\n",
    "    list_acc = [] #save sample3 random layers added \n",
    "    list_theta = []\n",
    "    list_A_save = []\n",
    "    list_Y_save = []\n",
    "\n",
    "    K = K + rand_layers #number layers\n",
    "    #list accuracy methods #for each random marix A \n",
    "    matrix_acc = np.zeros((K+5+2,sample1))  \n",
    "    #list values theta and alpha porposed methods \n",
    "    matrix_theta = np.zeros((sample1,2,K+1,C))\n",
    "    \n",
    "    for rrr in tqdm(range(0,sample3)): #random layers added \n",
    "        A = A_.copy()\n",
    "        \n",
    "        #generate weigthed random networks \n",
    "        n = 5 #space dimention\n",
    "        random_state = rrr #random sample\n",
    "        cluster_std = 8\n",
    "        for k in range(K-rand_layers,K):  \n",
    "            X_, labels_, centers_ = make_blobs(n_samples=N, centers=C, n_features=n, random_state=random_state+k,cluster_std=cluster_std, return_centers=True)\n",
    "            W_ = kneighbors_graph(X_, n_neighbors=knn, mode='distance', include_self=False)\n",
    "            W_ = 0.5 * (W_ + W_.T)\n",
    "            index = np.arange(N)\n",
    "            np.random.shuffle(index)\n",
    "            W_ = W_[index, :]\n",
    "            W_ = W_[:, index]\n",
    "            A.append(W_)\n",
    "           \n",
    "        #correspondent adjacency matrix - unweighted \n",
    "        for k in range(0,K):\n",
    "            A[k].data[:] = 1\n",
    "\n",
    "        # for k in range(0,K):\n",
    "        #     G = nx.from_scipy_sparse_matrix(A[k])\n",
    "        #     draw_adjacency_matrix(G)\n",
    "\n",
    "        Y_save = [] #save for state-of the-art-methods in Matlab\n",
    "        for r in tqdm(range(0,sample1)): #random indices know labels Y\n",
    "\n",
    "            # indices know labels each community \n",
    "            np.random.seed(r) \n",
    "            C_indexes_known = np.zeros(C, dtype=object)\n",
    "            for com in range(0,C):\n",
    "                C_indexes_known[com] = np.random.choice(C_indexes[com][0],labels_known_size[com],replace=False)\n",
    "\n",
    "            # Y matrix #known labels\n",
    "            C_indexes_known_union = np.concatenate(C_indexes_known)\n",
    "            comm_list = []\n",
    "            for c in range(0,C):\n",
    "                comm_list.append([c]*labels_known_size[c])\n",
    "            comm_list = np.concatenate(comm_list)\n",
    "            row = C_indexes_known_union\n",
    "            col = np.array(comm_list)\n",
    "            data = np.array(np.ones(sum(labels_known_size)))\n",
    "            Y = scipy.sparse.csr_matrix((data, (row, col)), shape=(N, C))\n",
    "            # normalized\n",
    "            Y = normalize(Y, norm='l2', axis=0) #check scipy.sparse.linalg.norm(Y, axis=0)\n",
    "\n",
    "            # expected communities (without known labels) #to calculate final accuracy\n",
    "            C_exp = np.delete(labels, obj=C_indexes_known_union)\n",
    "\n",
    "            Y_save.append(Y)\n",
    "\n",
    "            #acc_single_layers\n",
    "            for k in range(0,K):\n",
    "                theta = np.ones((1,2))[0]\n",
    "                sol_final = x_sol([A[k]],N,C,1,lam,theta,Y)\n",
    "                acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "                matrix_acc[k,r] = acc_k\n",
    "\n",
    "            #alpha=1 #arithmetic mean     \n",
    "            a = 1 #alpha 1\n",
    "            theta = np.ones(K+1) / K #theta sum 1 \n",
    "            theta[-1] = 1 #alpha 1\n",
    "            sol_final = x_sol(A,N,C,K,lam,theta,Y)\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[K,r]  = acc_k\n",
    "\n",
    "            #alpha=-1 #harmonic mean    \n",
    "            sol_final = scipy.sparse.lil_matrix((N,C))\n",
    "            theta = np.ones(K+1) / K #theta sum 1 \n",
    "            theta[-1] = -1 #alpha -1\n",
    "            sol_final = x_sol(A,N,C,K,lam,theta,Y)\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[K+1,r]  = acc_k\n",
    "\n",
    "            #alpha=+inf #maximum    \n",
    "            A_max = A[0]\n",
    "            for k in range(0,K-1):\n",
    "                A_max = A_max.maximum(A[k+1])\n",
    "            theta = np.ones((1,2))[0]\n",
    "            sol_final = x_sol([A_max],N,C,1,lam,theta,Y)\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[K+2,r]  = acc_k\n",
    "\n",
    "            #alpha=-inf #minimum    \n",
    "            A_min = A[0]\n",
    "            for k in range(0,K-1):\n",
    "                A_min = A_min.minimum(A[k+1])\n",
    "            theta = np.ones((1,2))[0]\n",
    "            sol_final = x_sol([A_min],N,C,1,lam,theta,Y)\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[K+3,r]  = acc_k\n",
    "\n",
    "            #alpha=0 #geometric mean    \n",
    "            A_prod = A[0]\n",
    "            for k in range(0,K-1):\n",
    "                A_prod = A_prod.multiply(A[k+1])\n",
    "            A_prod = scipy.sparse.csr_matrix.power(A_prod,1/K)\n",
    "            theta = np.ones((1,2))[0]\n",
    "            sol_final = x_sol([A_prod],N,C,1,lam,theta,Y)\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[K+4,r]  = acc_k \n",
    "\n",
    "            #proposed methods #parallelize sampling training and test sets\n",
    "            perc_ = 80 #perc_ = 50 #percentage test set\n",
    "            matrix_acc_0,sol_final_0,matrix_theta_0,matrix_acc_1,sol_final_1,matrix_theta_1 = zip(*Parallel(n_jobs=2,prefer=\"threads\")(delayed(methods)(A,N,K,C,lam,Y,labels_known_size,perc_,labels,C_indexes_known,sample2,rrr) for rrr in range(sample2)))\n",
    "\n",
    "            argmax0 = np.argmax(matrix_acc_0)\n",
    "            sol_final = sol_final_0[argmax0]\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[K+5,r]  = acc_k\n",
    "            matrix_theta[r,0,:,:]=matrix_theta_0[argmax0]\n",
    "\n",
    "            argmax1 = np.argmax(matrix_acc_1)\n",
    "            sol_final = sol_final_1[argmax1]\n",
    "            acc_k = accuracy_final(sol_final,C_indexes_known_union,C_exp)\n",
    "            matrix_acc[K+6,r]  = acc_k\n",
    "            matrix_theta[r,1,:,:]=matrix_theta_1[argmax1]\n",
    "                \n",
    "            #save in list \n",
    "            list_acc.append(matrix_acc)  \n",
    "            list_theta.append(matrix_theta)\n",
    "            list_A_save.append(A)\n",
    "            list_Y_save.append(Y_save)\n",
    "\n",
    "            #save matrix results samples \n",
    "            with open(\"Results_Real/Matlab_acc_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers)+'_rrr'+str(r),\"wb\") as fp:\n",
    "                pickle.dump(list_acc,fp)\n",
    "            with open(\"Results_Real/Matlab_theta_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers)+'_rrr'+str(r),\"wb\") as fp:\n",
    "                pickle.dump(list_theta,fp)\n",
    "            #save for state-of the-art-methods in Matlab \n",
    "            my_file = \"Results_Real/Datasets_Matlab_Real/Matlab_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers)+'_rrr'+str(r)+\".mat\"\n",
    "            scipy.io.savemat(my_file, dict(A_list=list_A_save, Y_list=list_Y_save, labels_list=labels))\n",
    "                \n",
    "    #save matrix results samples \n",
    "    with open(\"Results_Real/Matlab_acc_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers),\"wb\") as fp:\n",
    "        pickle.dump(list_acc,fp)\n",
    "    with open(\"Results_Real/Matlab_theta_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers),\"wb\") as fp:\n",
    "        pickle.dump(list_theta,fp)\n",
    "    #save for state-of the-art-methods in Matlab \n",
    "    my_file = \"Results_Real/Datasets_Matlab_Real/Matlab_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers)+\".mat\"\n",
    "    scipy.io.savemat(my_file, dict(A_list=list_A_save, Y_list=list_Y_save, labels_list=labels))\n",
    "\n",
    "    return list_A_save,labels,list_Y_save,list_acc,list_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edaf04-807a-4095-b699-be1aeda63e78",
   "metadata": {},
   "source": [
    "### results_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb80252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_statistics_synthetic(N,C,K,case,cluster_std,knn,perc,lam):\n",
    "    \n",
    "    sample1 = 5\n",
    "    \n",
    "    #download matrix\n",
    "    with open('Results/matrix_acc_theta_N'+str(N)+'_C'+str(C)+'_case '+case+'_std'+str(cluster_std)+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_sample'+str(sample1),\"rb\") as fp:\n",
    "        matrix_acc = pickle.load(fp)\n",
    "    with open('Results/matrix_theta_theta_N'+str(N)+'_C'+str(C)+'_case '+case+'_std'+str(cluster_std)+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_sample'+str(sample1),\"rb\") as fp:\n",
    "        matrix_theta = pickle.load(fp)\n",
    "        \n",
    "    K = len(matrix_acc)-(5+2) #layers  \n",
    "    \n",
    "    method_names = []\n",
    "    for i in range(0,K):\n",
    "        method_names.append('layer '+str(i+1))\n",
    "    method_names = method_names + ['arithmetic', \n",
    "    'harmonic',\n",
    "    'maximum',\n",
    "    'minimum',\n",
    "    'geometric',\n",
    "    'binomial',\n",
    "    'multi']\n",
    "\n",
    "    #mean and std all methods respect to random networks\n",
    "    matrix_acc_mean = np.mean(matrix_acc,axis=1)\n",
    "    matrix_acc_std = np.std(matrix_acc,axis=1)\n",
    "    \n",
    "    matrix_acc_mean_df = pd.DataFrame(list(zip(method_names,matrix_acc_mean,matrix_acc_std)))\n",
    "    matrix_acc_mean_df.columns = ['method', 'acc_mean','acc_std']\n",
    "    #matrix_acc_mean_df = matrix_acc_mean_df.sort_values(by=['acc_mean'], ascending=False)\n",
    "    \n",
    "\n",
    "    #mean and std all methods respect to random networks\n",
    "    matrix_theta_mean = np.mean(matrix_theta,axis=0)\n",
    "    matrix_theta_std = np.std(matrix_theta,axis=0)\n",
    "\n",
    "    df1 = pd.DataFrame(list(matrix_theta_mean[0]))\n",
    "    df1.insert(0, 'method/community', method_names[-2])\n",
    "    df2 = pd.DataFrame(list(matrix_theta_mean[1]))\n",
    "    df2.insert(0, 'method/community', method_names[-1])\n",
    "\n",
    "    matrix_theta_mean_df = pd.concat([df1, df2], axis = 0)\n",
    "    \n",
    "    return matrix_acc_mean_df,matrix_theta_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6cba9e-f89c-4930-a09b-975aa5ba09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_statistics_real(dataset,knn,perc,lam):\n",
    "    \n",
    "    sample1 = 5\n",
    "    \n",
    "    #download matrix\n",
    "    with open(\"Results_Real/Matlab_acc_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam),\"rb\") as fp:\n",
    "        matrix_acc = pickle.load(fp)\n",
    "    with open(\"Results_Real/Matlab_theta_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam),\"rb\") as fp:\n",
    "        matrix_theta = pickle.load(fp)\n",
    "        \n",
    "    K = len(matrix_acc)-(5+2) #layers  \n",
    "    \n",
    "    method_names = []\n",
    "    for i in range(0,K):\n",
    "        method_names.append('layer '+str(i+1))\n",
    "    method_names = method_names + ['arithmetic', \n",
    "    'harmonic',\n",
    "    'maximum',\n",
    "    'minimum',\n",
    "    'geometric',\n",
    "    'binomial',\n",
    "    'multi']\n",
    "\n",
    "    #mean and std all methods respect to random networks\n",
    "    matrix_acc_mean = np.mean(matrix_acc,axis=1)\n",
    "    matrix_acc_std = np.std(matrix_acc,axis=1)\n",
    "    \n",
    "    matrix_acc_mean_df = pd.DataFrame(list(zip(method_names,matrix_acc_mean,matrix_acc_std)))\n",
    "    matrix_acc_mean_df.columns = ['method', 'acc_mean','acc_std']\n",
    "    #matrix_acc_mean_df = matrix_acc_mean_df.sort_values(by=['acc_mean'], ascending=False)\n",
    "    \n",
    "\n",
    "    #mean and std all methods respect to random networks\n",
    "    matrix_theta_mean = np.mean(matrix_theta,axis=0)\n",
    "    matrix_theta_std = np.std(matrix_theta,axis=0)\n",
    "    \n",
    "    df1 = pd.DataFrame(list(matrix_theta_mean[1]))\n",
    "    df1.insert(0, 'method/community', method_names[-2])\n",
    "    df2 = pd.DataFrame(list(matrix_theta_mean[2]))\n",
    "    df2.insert(0, 'method/community', method_names[-1])\n",
    "\n",
    "    matrix_theta_mean_df = pd.concat([df1, df2], axis = 0)\n",
    "    \n",
    "    return matrix_acc_mean_df,matrix_theta_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e349c248-6801-4a4b-9ca0-9e58b37b2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict number communities\n",
    "datasets_list = [\"3sources\",\"BBCSport2view_544\",\"BBC4view_685\",\"WikipediaArticles\",\"UCI_mfeat\",\"cora\",\"citeseer\",\"dkpol\",\"aucs\"]\n",
    "dict_dataset_C = dict(zip(datasets_list,[6,5,5,10,10,7,6,10]))\n",
    "\n",
    "def results_statistics_real_noisy(dataset,knn,perc,lam,rand_layers):\n",
    "    \n",
    "    #download matrix\n",
    "    with open(\"Results_Real/Matlab_acc_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers),\"rb\") as fp:\n",
    "        list_acc = pickle.load(fp)\n",
    "    with open(\"Results_Real/Matlab_theta_theta_\"+dataset+'_knn'+str(knn)+'_perc'+str(perc)+'_lam'+str(lam)+'_Knoisy'+str(rand_layers),\"rb\") as fp:\n",
    "        list_theta = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "    sample3 = len(list_acc) #random layers added\n",
    "    \n",
    "    K = len(list_acc[0])-(5+2) #layers \n",
    "    \n",
    "    method_names = []\n",
    "    for i in range(0,K):\n",
    "        method_names.append('layer '+str(i+1))\n",
    "    method_names = method_names + ['arithmetic', \n",
    "    'harmonic',\n",
    "    'maximum',\n",
    "    'minimum',\n",
    "    'geometric',\n",
    "    'binomial',\n",
    "    'multi']\n",
    "    \n",
    "    matrix_acc_mean_ = np.empty([K+5+2,sample3])\n",
    "    matrix_acc_std_ = np.empty([K+5+2,sample3])\n",
    "    for rrr in range(0,sample3): #random layers added \n",
    "\n",
    "        matrix_acc = list_acc[rrr]\n",
    "\n",
    "        #mean and std all methods respect to random networks\n",
    "        matrix_acc_mean = np.mean(matrix_acc,axis=1)\n",
    "        matrix_acc_std = np.std(matrix_acc,axis=1)\n",
    "\n",
    "        matrix_acc_mean_[:,rrr] = matrix_acc_mean\n",
    "        matrix_acc_std_[:,rrr] = matrix_acc_std\n",
    "\n",
    "    matrix_acc_mean = np.mean(matrix_acc_mean_,axis=1)\n",
    "    matrix_acc_std = np.mean(matrix_acc_std_,axis=1)\n",
    "\n",
    "    matrix_acc_mean_df = pd.DataFrame(list(zip(method_names,matrix_acc_mean,matrix_acc_std)))\n",
    "    matrix_acc_mean_df.columns = ['method', 'acc_mean','acc_std']\n",
    "    #matrix_acc_mean_df = matrix_acc_mean_df.sort_values(by=['acc_mean'], ascending=False)\n",
    "    \n",
    "    C = dict_dataset_C[dataset]\n",
    "    matrix_theta_mean_ = np.empty([sample3,2,K+1,C])\n",
    "    matrix_theta_std_ = np.empty([sample3,2,K+1,C])\n",
    "    for rrr in range(0,sample3): #random layers added\n",
    "\n",
    "        matrix_theta = list_theta[rrr]\n",
    "\n",
    "        #mean and std all methods respect to random networks\n",
    "        matrix_theta_mean = np.mean(matrix_theta,axis=0)\n",
    "        matrix_theta_std = np.std(matrix_theta,axis=0)\n",
    "\n",
    "        matrix_theta_mean_[rrr,:,:,:] = matrix_theta_mean\n",
    "        matrix_theta_std_[rrr,:,:,:] = matrix_theta_std\n",
    "\n",
    "    matrix_theta_mean = np.mean(matrix_theta_mean_,axis=0)\n",
    "    matrix_theta_std = np.mean(matrix_theta_std_,axis=0)\n",
    "\n",
    "    df1 = pd.DataFrame(list(matrix_theta_mean[0]))\n",
    "    df1.insert(0, 'method/community', method_names[-2])\n",
    "    df2 = pd.DataFrame(list(matrix_theta_mean[1]))\n",
    "    df2.insert(0, 'method/community', method_names[-1])\n",
    "\n",
    "    matrix_theta_mean_df = pd.concat([df1, df2], axis = 0)\n",
    "    \n",
    "    \n",
    "    return matrix_acc_mean_df,matrix_theta_mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68c608-12e3-4040-b02f-8c5a8f71a2ba",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2a425-d218-48e9-bc2c-8c5713ce5737",
   "metadata": {},
   "source": [
    "#### synthetic_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5fc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1200 #number of nodes\n",
    "C = 3 #number of communities\n",
    "K = 3 #number layers\n",
    "perc = 20\n",
    "lam = 1 #lambda #parameter lower level pbl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587048c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'info2' \n",
    "knn = 5 \n",
    "for cluster_std in [5,6,7,8]:\n",
    "    A_save,labels_save,Y_save,matrix_acc,matrix_theta = synthetic_datasets(N,C,K,case,cluster_std,knn,perc,lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6678e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'compl' \n",
    "knn = 5 \n",
    "for cluster_std in [2,3,4,5]:\n",
    "    A_save,labels_save,Y_save,matrix_acc,matrix_theta = synthetic_datasets(N,C,K,case,cluster_std,knn,perc,lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3df57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'noisy' #info2,noisy,compl,compl2\n",
    "knn = 5 \n",
    "for cluster_std in [2,3,4,5]:\n",
    "    A_save,labels_save,Y_save,matrix_acc,matrix_theta = synthetic_datasets(N,C,K,case,cluster_std,knn,perc,lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc1ed9-5a60-497a-be18-10cb74fbf289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b2955-3844-43f2-b621-41a4a953a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310468d2-5334-4c1b-9726-ae7cfc441594",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'info2' \n",
    "print(\"case \"+case)\n",
    "knn = 5 \n",
    "for cluster_std in [5,6,7,8]:\n",
    "    print(\"std \"+str(cluster_std))\n",
    "    matrix_acc_mean,matrix_theta_mean = results_statistics_synthetic(N,C,K,case,cluster_std,knn,perc,lam)\n",
    "    display(matrix_acc_mean)\n",
    "    display(matrix_theta_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6752c-4a71-41de-8b3e-8c890c026439",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'noisy' #info2,noisy,compl,compl2\n",
    "print(\"case \"+case)\n",
    "knn = 5 \n",
    "for cluster_std in [2,3,4,5]:\n",
    "    print(\"std \"+str(cluster_std))\n",
    "    matrix_acc_mean,matrix_theta_mean = results_statistics_synthetic(N,C,K,case,cluster_std,knn,perc,lam)\n",
    "    display(matrix_acc_mean)\n",
    "    display(matrix_theta_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80462fef-1d7d-4fbc-9280-b0bb3958538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'compl' \n",
    "print(\"case \"+case)\n",
    "knn = 5 \n",
    "for cluster_std in [2,3,4,5]:\n",
    "    print(\"std \"+str(cluster_std))\n",
    "    matrix_acc_mean,matrix_theta_mean = results_statistics_synthetic(N,C,K,case,cluster_std,knn,perc,lam)\n",
    "    display(matrix_acc_mean)\n",
    "    display(matrix_theta_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5d899-b7c4-44b7-9865-6d25fea7851f",
   "metadata": {},
   "source": [
    "#### real_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a1784-5572-4148-89b8-637885b01b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = 5\n",
    "lam = 1 #lambda #parameter lower level pbl\n",
    "for dataset in datasets_list:\n",
    "    for perc in [15]: #[1,5,10,15,20,25]\n",
    "        A,labels,Y_save,matrix_acc,matrix_theta  = real_datasets(dataset,knn,perc,lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd239b-a09b-4dfd-8d98-454f73ea42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results\n",
    "knn = 5\n",
    "lam = 1 \n",
    "perc = 15\n",
    "for dataset in datasets_list:\n",
    "        print(dataset)\n",
    "        matrix_acc_mean_df,matrix_theta_mean_df = results_statistics_real(dataset,knn,perc,lam)\n",
    "        display(matrix_acc_mean_df)\n",
    "        display(matrix_theta_mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762b081-ae69-4a99-bfa5-9a6ecf5d4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cea73-949f-453a-b50e-2c413eb5e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = 5\n",
    "lam = 1 #lambda #parameter lower level pbl\n",
    "perc = 15\n",
    "for dataset in datasets_list:\n",
    "    for rand_layers in [1,2]: \n",
    "        A_save,labels,Y_save,list_acc,list_theta  = real_datasets_noisy(dataset,knn,perc,lam,rand_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46831a73-5f91-4cd7-b8d5-36da160830c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = 5\n",
    "lam = 1 #lambda #parameter lower level pbl\n",
    "perc = 15\n",
    "for dataset in datasets_list:\n",
    "    for rand_layers in [1,2]: \n",
    "        print(dataset+\" - \"+str(rand_layers)+\" random layers\")\n",
    "        matrix_acc_mean_df,matrix_theta_mean_df = results_statistics_real_noisy(dataset,knn,perc,lam,rand_layers)\n",
    "        display(matrix_acc_mean_df)\n",
    "        display(matrix_theta_mean_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
